{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3896587a-586f-449b-8ed1-ebcc3a34adda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "token = os.environ.get('DATABRICKS_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67fc44b-0c99-43e8-802c-0bf642edaa4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Policies"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Databricks configuration\n",
    "DATABRICKS_INSTANCE = \"https://adb-201068313543333.13.azuredatabricks.net\"\n",
    "DATABRICKS_TOKEN = {token}\n",
    "\n",
    "def get_all_policies():\n",
    "    \"\"\"Extract all cluster policies from Databricks workspace\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Get all policies\n",
    "    response = requests.get(\n",
    "        f\"{DATABRICKS_INSTANCE}/api/2.0/policies/clusters/list\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch policies: {response.text}\")\n",
    "    \n",
    "    policies_data = response.json()\n",
    "    \n",
    "    # Get detailed policy definitions\n",
    "    policies_with_definitions = []\n",
    "    for policy in policies_data.get('policies', []):\n",
    "        policy_id = policy['policy_id']\n",
    "        \n",
    "        # Get policy definition\n",
    "        def_response = requests.get(\n",
    "            f\"{DATABRICKS_INSTANCE}/api/2.0/policies/clusters/get\",\n",
    "            headers=headers,\n",
    "            params={\"policy_id\": policy_id}\n",
    "        )\n",
    "        \n",
    "        if def_response.status_code == 200:\n",
    "            policy_detail = def_response.json()\n",
    "            policies_with_definitions.append(policy_detail)\n",
    "        else:\n",
    "            print(f\"Warning: Could not fetch details for policy {policy_id}\")\n",
    "    \n",
    "    return policies_with_definitions\n",
    "\n",
    "def convert_policy_to_bundle_format(policy):\n",
    "    \"\"\"Convert Databricks policy to Asset Bundle format\"\"\"\n",
    "    policy_name = policy['name'].lower().replace(' ', '-').replace('_', '-')\n",
    "    \n",
    "    bundle_policy = {\n",
    "        'name': policy['name'],\n",
    "        'definition': policy.get('definition', {})\n",
    "    }\n",
    "    \n",
    "    # Add description if exists\n",
    "    if policy.get('description'):\n",
    "        bundle_policy['description'] = policy['description']\n",
    "    \n",
    "    return policy_name, bundle_policy\n",
    "\n",
    "def generate_bundle_yaml(policies):\n",
    "    \"\"\"Generate complete Databricks Asset Bundle YAML\"\"\"\n",
    "    \n",
    "    # Convert policies to bundle format\n",
    "    bundle_policies = {}\n",
    "    for policy in policies:\n",
    "        policy_name, bundle_policy = convert_policy_to_bundle_format(policy)\n",
    "        bundle_policies[policy_name] = bundle_policy\n",
    "    \n",
    "    # Policies resources\n",
    "    policies_config = {\n",
    "        'resources': {\n",
    "            'policies': bundle_policies\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return  policies_config\n",
    "\n",
    "def main():\n",
    "    print(\"Extracting policies from Databricks workspace...\")\n",
    "    \n",
    "    # Get all policies\n",
    "    policies = get_all_policies()\n",
    "    print(f\"Found {len(policies)} policies\")\n",
    "    \n",
    "    # Generate YAML configurations\n",
    "    policies_config = generate_bundle_yaml(policies)\n",
    "    \n",
    "    # Create directory structur\n",
    "    \n",
    "    # Write policies file\n",
    "    with open('../resources/SharedObjects/policies.yml', 'w') as f:\n",
    "        yaml.dump(policies_config, f, default_flow_style=False, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Policy extraction completed!\")\n",
    "    print(\"üìÅ Files created:\")\n",
    "    print(\"   - resources/policies.yml\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326b8935-2fc3-4ba8-818c-77f9f622c6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def get_databricks_instance():\n",
    "    return \"https://adb-201068313543333.13.azuredatabricks.net\"\n",
    "\n",
    "def get_databricks_token():\n",
    "    return token\n",
    "\n",
    "def get_all_clusters():\n",
    "    \"\"\"Extract all active clusters from Databricks workspace\"\"\"\n",
    "    instance = get_databricks_instance()\n",
    "    token = get_databricks_token()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(\n",
    "        f\"{instance}/api/2.0/clusters/list\",\n",
    "        headers=headers\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch clusters: {response.text}\")\n",
    "    clusters_data = response.json()\n",
    "    # Only return clusters that are not job clusters\n",
    "    return [c for c in clusters_data.get('clusters', []) if c.get('cluster_source') != 'JOB']\n",
    "\n",
    "def convert_cluster_to_bundle_format(cluster):\n",
    "    \"\"\"Convert Databricks cluster to Asset Bundle format\"\"\"\n",
    "    cluster_name = cluster['cluster_name'].lower().replace(' ', '-').replace('_', '-')\n",
    "    bundle_cluster = {\n",
    "        'cluster_name': cluster['cluster_name'],\n",
    "        'spark_version': cluster.get('spark_version'),\n",
    "        'node_type_id': cluster.get('node_type_id'),\n",
    "        'num_workers': cluster.get('num_workers'),\n",
    "        'autotermination_minutes': cluster.get('autotermination_minutes'),\n",
    "        'custom_tags': cluster.get('custom_tags', {}),\n",
    "        'cluster_log_conf': cluster.get('cluster_log_conf', {}),\n",
    "        'init_scripts': cluster.get('init_scripts', []),\n",
    "        'aws_attributes': cluster.get('aws_attributes', {}),\n",
    "        'azure_attributes': cluster.get('azure_attributes', {}),\n",
    "        'gcp_attributes': cluster.get('gcp_attributes', {}),\n",
    "        'policy_id': cluster.get('policy_id')\n",
    "    }\n",
    "    # Remove empty fields\n",
    "    bundle_cluster = {k: v for k, v in bundle_cluster.items() if v}\n",
    "    return cluster_name, bundle_cluster\n",
    "\n",
    "def get_all_sql_warehouses():\n",
    "    \"\"\"Extract all SQL warehouses from Databricks workspace\"\"\"\n",
    "    instance = get_databricks_instance()\n",
    "    token = get_databricks_token()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(\n",
    "        f\"{instance}/api/2.0/sql/warehouses\",\n",
    "        headers=headers\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch SQL warehouses: {response.text}\")\n",
    "    warehouses_data = response.json()\n",
    "    return warehouses_data.get('warehouses', [])\n",
    "\n",
    "def convert_sql_warehouse_to_bundle_format(warehouse):\n",
    "    \"\"\"Convert Databricks SQL warehouse to Asset Bundle format\"\"\"\n",
    "    warehouse_name = warehouse['name'].lower().replace(' ', '-').replace('_', '-')\n",
    "    bundle_warehouse = {\n",
    "        'name': warehouse['name'],\n",
    "        'cluster_size': warehouse.get('cluster_size'),\n",
    "        'min_num_clusters': warehouse.get('min_num_clusters'),\n",
    "        'max_num_clusters': warehouse.get('max_num_clusters'),\n",
    "        'auto_stop_mins': warehouse.get('auto_stop_mins'),\n",
    "        'enable_serverless_compute': warehouse.get('enable_serverless_compute'),\n",
    "        'spot_instance_policy': warehouse.get('spot_instance_policy'),\n",
    "        'tags': warehouse.get('tags', {}),\n",
    "        'channel': warehouse.get('channel'),\n",
    "        'warehouse_type': warehouse.get('warehouse_type')\n",
    "    }\n",
    "    # Remove empty fields\n",
    "    bundle_warehouse = {k: v for k, v in bundle_warehouse.items() if v is not None}\n",
    "    return warehouse_name, bundle_warehouse\n",
    "\n",
    "def generate_clusters_bundle_yaml(clusters):\n",
    "    \"\"\"Generate Databricks Asset Bundle YAML for clusters\"\"\"\n",
    "    bundle_clusters = {}\n",
    "    for cluster in clusters:\n",
    "        # Exclude job clusters\n",
    "        if cluster.get('cluster_source') == 'JOB':\n",
    "            continue\n",
    "        cluster_name, bundle_cluster = convert_cluster_to_bundle_format(cluster)\n",
    "        # Separate by cluster type\n",
    "        cluster_type = cluster.get('cluster_source', 'ALL_PURPOSE')\n",
    "        if cluster_type != 'JOB':\n",
    "            bundle_clusters[cluster_name] = bundle_cluster\n",
    "    if bundle_clusters:\n",
    "        clusters_config = {\n",
    "            'resources': {\n",
    "                'clusters': bundle_clusters\n",
    "            }\n",
    "        }\n",
    "        return clusters_config\n",
    "    return None\n",
    "\n",
    "def generate_job_clusters_bundle_yaml(clusters):\n",
    "    \"\"\"Generate Databricks Asset Bundle YAML for job clusters (not needed)\"\"\"\n",
    "    return None  # Not needed as per user request\n",
    "\n",
    "def generate_sql_warehouses_bundle_yaml(warehouses):\n",
    "    \"\"\"Generate Databricks Asset Bundle YAML for SQL warehouses\"\"\"\n",
    "    bundle_warehouses = {}\n",
    "    for warehouse in warehouses:\n",
    "        warehouse_name, bundle_warehouse = convert_sql_warehouse_to_bundle_format(warehouse)\n",
    "        bundle_warehouses[warehouse_name] = bundle_warehouse\n",
    "    if bundle_warehouses:\n",
    "        warehouses_config = {\n",
    "            'resources': {\n",
    "                'sql_warehouses': bundle_warehouses\n",
    "            }\n",
    "        }\n",
    "        return warehouses_config\n",
    "    return None\n",
    "\n",
    "print(\"Extracting clusters and SQL warehouses from Databricks workspace...\")\n",
    "clusters = get_all_clusters()\n",
    "for row in clusters:\n",
    "    if row.get('cluster_source') != 'JOB':\n",
    "        print(row['cluster_source'])\n",
    "warehouses = get_all_sql_warehouses()\n",
    "print(f\"Found {len(clusters)} clusters\")\n",
    "print(f\"Found {len(warehouses)} SQL warehouses\")\n",
    "os.makedirs('../resources/SharedObjects', exist_ok=True)\n",
    "# All-purpose clusters\n",
    "clusters_config = generate_clusters_bundle_yaml(clusters)\n",
    "if clusters_config:\n",
    "    with open('../resources/SharedObjects/all_purpose_clusters.yml', 'w') as f:\n",
    "        yaml.dump(clusters_config, f, default_flow_style=False, indent=2)\n",
    "    print(\"   - resources/all_purpose_clusters.yml\")\n",
    "# SQL warehouses\n",
    "warehouses_config = generate_sql_warehouses_bundle_yaml(warehouses)\n",
    "if warehouses_config:\n",
    "    with open('../resources/SharedObjects/sql_warehouses.yml', 'w') as f:\n",
    "        yaml.dump(warehouses_config, f, default_flow_style=False, indent=2)\n",
    "    print(\"   - resources/sql_warehouses.yml\")\n",
    "print(\"‚úÖ Extraction completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prerequisites",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
